## 1. NLTK and Basic Corpus Analysis (Frequency Distribution, Morphology, Word Clouds)

### Objective
Provide comprehensive theory, terminology, practical notes, and extensive viva questions with concise answers for Experiment 1: installing NLTK and performing corpus analysis including frequency distributions, morphological analysis, and word cloud generation.

---

## Theory and Concepts

1. NLTK overview
- NLTK (Natural Language Toolkit) is a Python library for NLP research and education. It includes tokenizers, corpus readers, POS taggers, parsers, WordNet interface, and utilities for classification and evaluation.
- Installation: typically via pip: `pip install nltk`. Many corpora/models must be downloaded using `nltk.download()` (e.g., 'punkt', 'averaged_perceptron_tagger', 'wordnet', 'stopwords').

2. Corpora and corpus readers
- Common corpora: Gutenberg, Brown, Penn Treebank, Reuters, WordNet, movie_reviews, stopwords.
- Corpus readers provide standardized access to raw text, tokenized words, tagged text, and categorized files.

3. Tokenization
- Purpose: split raw text into tokens (words, sentences). Sentence tokenizers (Punkt) and word tokenizers (TreebankWordTokenizer, word_tokenize) exist.
- Issues: punctuation, contractions, multilingual text, clitics, hyphenation.

4. Frequency distribution (FreqDist)
- FreqDist from NLTK computes term frequencies and supports methods like .most_common(), .plot(), and cumulative frequency analysis.
- ConditionalFreqDist supports frequencies conditioned on categories (e.g., by genre, by author).

5. Morphological analysis
- Morphology studies word formation: stems, roots, prefixes, suffixes, inflectional vs derivational morphology.
- Tools: Stemmer (Porter, Lancaster, Snowball), Lemmatizer (WordNetLemmatizer) which uses POS tags for accurate lemmatization.
- POS tagging (part-of-speech): assigns syntactic categories (NN, VB, JJ). NLTK provides pretrained taggers (averaged perceptron, unigram/bigram taggers, regex taggers).

6. WordNet and semantic relations
- WordNet is a lexical database providing synsets (sets of synonyms), hypernyms/hyponyms, antonyms, example sentences, and morphological functions.
- Use WordNet for lemmatization, synonym lookup, and simple semantic similarity.

7. Collocations and n-grams
- Collocations: frequent lexical combinations (e.g., "United States"). Use bigrams/trigrams and association measures like PMI (pointwise mutual information).
- N-grams capture local co-occurrence patterns used for language modeling and collocation detection.

8. Concordance and context analysis
- Concordance lists occurrences of a word with surrounding context (useful to study senses and usage patterns).

9. Word clouds
- Visual summarization where word size encodes frequency. Useful for quick EDA but not rigorous quantitative analysis.
- Libraries: wordcloud (Python) accepts frequency dictionaries to render images; tune parameters: max_words, background_color, stopwords.

10. Stopwords and normalization
- Stopwords: common words often removed (e.g., 'the', 'is'). Normalization includes lowercasing, punctuation removal, unicode normalization, and expanding contractions.

11. Zipf's law and frequency distributions
- Zipf's law: term frequency inversely proportional to rank; few words are very frequent, many are rare. Visualize with rank-frequency plots (log-log).

12. TF-IDF and weighting (brief)
- TF-IDF useful for weighting terms by importance in documents; not native to NLTK but available via scikit-learn.

13. Evaluation and quality checks
- Validate tokenization and POS tagging quality via sample inspection. For tagger accuracy, compare predicted tags against annotated corpora (e.g., Penn Treebank) and compute accuracy, precision/recall for tasks like chunking or named entity recognition.

---

## Practical & Implementation Notes

- Installing NLTK and corpora:
	- pip install nltk
	- python -m nltk.downloader punkt wordnet averaged_perceptron_tagger stopwords
- Typical workflow:
	- Load corpus or text
	- Preprocess (lowercase, remove punctuation or normalize)
	- Tokenize (sentences → words)
	- Optionally remove stopwords
	- Build FreqDist, ConditionalFreqDist, or compute n-grams
	- POS-tag tokens and apply lemmatization/stemming
	- Create word cloud from frequencies
- Tips:
	- Use list comprehensions and generator expressions for memory efficiency.
	- For large corpora, process in streams and incrementally update FreqDist.
	- Save outputs (freq dicts, plots) for reproducibility.

---

## Terminologies (definitions)

- Tokenization: splitting text into tokens.
- Sentence tokenization: splitting text into sentences.
- Corpus: a body of text used for linguistic analysis.
- FreqDist: frequency distribution of tokens.
- ConditionalFreqDist: frequency distribution conditioned by a variable (e.g., genre).
- Lemma: base/dictionary form of a word.
- Stem: truncated root form produced by a stemmer.
- POS tag: part-of-speech label (e.g., NN, VB, JJ).
- Stopwords: common words often removed before analysis.
- Collocation: commonly co-occurring word sequence.
- N-gram: contiguous sequence of n tokens.
- WordNet synset: set of cognitive synonyms representing one concept.
- PMI (Pointwise Mutual Information): measure of association between words.
- Zipf's law: empirical law relating frequency and rank of words.
- Lemmatizer vs Stemmer: lemmatizer uses vocabulary/morphology; stemmer applies heuristic truncation.

---

## Viva Questions (grouped) — without answers (kept for practice)

### Basic / Definitions
1. What is NLTK and why is it used?
2. How do you install NLTK and its data corpora?
3. Define corpus and give examples of corpora available in NLTK.
4. What is tokenization? Name two tokenizers provided by NLTK.
5. What is a frequency distribution (FreqDist)? How is it useful?
6. What is a stopword and why remove it?
7. What is stemming? Name two stemmers.
8. What is lemmatization? How does it differ from stemming?
9. What is POS tagging? Why is it useful for lemmatization?
10. What is WordNet and what can it provide?

### Conceptual / Explanatory
11. How would you use FreqDist to find the most common words in a corpus?
12. Explain ConditionalFreqDist with an example.
13. How do you detect collocations in a text? Name measures used.
14. Explain Zipf's law and how to visualize it.
15. What preprocessing steps do you perform before building a word cloud?
16. When is stemming preferable to lemmatization and vice versa?
17. How would you handle punctuation, numbers, and special tokens in corpus analysis?
18. How can POS tags improve morphological analysis?
19. Why should you be cautious interpreting word clouds?
20. How can you compute bigrams and trigrams using NLTK?

### Implementation / Practical
21. Show code to compute a FreqDist for a text and plot the top 20 words.
22. How do you generate a word cloud from a frequency dictionary?
23. How to download NLTK data programmatically in a script?
24. How to lemmatize words with correct POS tags in NLTK?
25. How to build and inspect a ConditionalFreqDist by genre or category?

### Evaluation & Debugging
26. How to evaluate the accuracy of a POS tagger?
27. What to do if tokenizer splits contractions incorrectly (e.g., "don't" → "do" "n't")?
28. How to handle multilingual text and encodings when using NLTK?
29. If you see very frequent punctuation tokens in FreqDist, what might be wrong?
30. How to handle very large corpora that don't fit in memory?

### Advanced / Research-style
31. How to compute PMI for collocations and interpret it?
32. How can WordNet be used to expand queries in information retrieval?
33. How to use regular expressions for custom tokenization in NLTK?
34. Explain combining NLTK preprocessing with scikit-learn TF-IDF pipelines.
35. Describe methods to detect and handle noisy or mislabeled data in corpora.

---

## Viva Answers (concise)

### Basic / Definitions
1. Answer: NLTK is a Python toolkit for NLP tasks (tokenization, tagging, parsing, corpora access) used for learning and prototyping.
2. Answer: pip install nltk; then use `nltk.download()` or `python -m nltk.downloader` to fetch models/data (e.g., 'punkt', 'wordnet').
3. Answer: Corpus = dataset of text; examples: Gutenberg, Brown, Reuters, Penn Treebank, WordNet.
4. Answer: Tokenization splits text into tokens; tokenizers: word_tokenize (Punkt), TreebankWordTokenizer, regexp_tokenize.
5. Answer: FreqDist counts token occurrences and helps identify common/rare words, visualize distributions, and build stoplists.
6. Answer: Stopwords are high-frequency function words ("the", "and"); removing them reduces noise but may drop signals.
7. Answer: Stemming reduces words to stem forms; examples: PorterStemmer, LancasterStemmer, SnowballStemmer.
8. Answer: Lemmatization returns dictionary/base forms (lemmas) using morphological analysis and POS (WordNetLemmatizer).
9. Answer: POS tagging assigns syntactic categories (e.g., NN, VB); useful for disambiguating lemmas and for downstream tasks.
10. Answer: WordNet provides synsets, definitions, examples, and relationships (synonymy, hypernymy) useful for semantics and lemmatization.

### Conceptual / Explanatory
11. Answer: Use FreqDist(tokens); call .most_common(20) and .plot(20) to visualize top words.
12. Answer: ConditionalFreqDist maps a condition (e.g., genre) to FreqDist; example: counts of words per Brown corpus category.
13. Answer: Use BigramCollocationFinder, apply score functions like PMI or likelihood ratio to rank collocations.
14. Answer: Zipf's law: frequency ≈ constant / rank; plot log(rank) vs log(frequency) to see near-linear relation.
15. Answer: Preprocess by lowercasing, removing stopwords and punctuation, normalizing tokens, and handling numbers before generating a word cloud.
16. Answer: Stemming is faster and language-agnostic but crude; lemmatization is accurate but needs POS and resources.
17. Answer: Remove or normalize punctuation, map numbers to a token or keep as-is depending on task, use regex to identify URLs/emails.
18. Answer: POS tags let lemmatizers pick the correct lemma (e.g., "better" → 'good' as adjective vs verb forms).
19. Answer: Word clouds show frequency but not context or importance; they can mislead if stopwords or rare but important words are ignored.
20. Answer: Use nltk.ngrams or BigramCollocationFinder; compute and count n-grams from token lists.

### Implementation / Practical
21. Answer (concept): tokens = word_tokenize(text); fd = FreqDist(tokens); fd.plot(20)
22. Answer (concept): from wordcloud import WordCloud; wc = WordCloud().generate_from_frequencies(freq_dict); wc.to_file('wc.png')
23. Answer: Use `nltk.download('punkt')` etc. in script or `python -m nltk.downloader punkt wordnet stopwords`.
24. Answer: POS-tag: tags = pos_tag(tokens); map NLTK tags to WordNet tags and call WordNetLemmatizer.lemmatize(token, pos)
25. Answer: Iterate files by category, build Cfd: cfd = ConditionalFreqDist((category, word) for (category, words) in corpus)

### Evaluation & Debugging
26. Answer: Compare tagger output vs gold standard annotated corpus and compute accuracy; use confusion matrix for tag errors.
27. Answer: Use Treebank tokenizer or regex that preserves contractions; normalize tokens to expected formats.
28. Answer: Ensure correct encoding (utf-8), detect language with langid or fastText, use language-specific tokenizers.
29. Answer: Probably didn't remove punctuation before tokenizing or used character-level tokenization; inspect tokenizer choice.
30. Answer: Stream processing, incremental FreqDist updates, or use corpus readers that yield documents one-by-one.

### Advanced / Research-style
31. Answer: PMI(w1,w2) = log [ P(w1,w2) / (P(w1)P(w2)) ]; high PMI indicates strong association but can be biased toward low-frequency pairs.
32. Answer: Expand queries by adding synonyms/hypernyms from WordNet to improve recall in IR; be careful with precision.
33. Answer: Use regexp_tokenize with custom regex patterns or build RegexTokenizer to capture domain-specific tokens.
34. Answer: Preprocess with NLTK (tokenize, normalize), then feed token lists to scikit-learn's CountVectorizer/TfidfVectorizer with custom analyzer.
35. Answer: Use heuristics, cross-validation, and manual review; apply noise detection models or active learning to prioritize relabeling.

---

End of Experiment 1 viva content: NLTK and Basic Corpus Analysis.
